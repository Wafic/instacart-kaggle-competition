{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following is heavily influenced by this [blogpost](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waficel-assi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#import data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#data types\n",
    "import collections\n",
    "\n",
    "#import general libraries\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "#deep learning \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_products = pd.read_csv('./data/subset_products_list_july30.csv')\n",
    "df_product_embeddings = pd.read_csv('./data/subset_data_for_embeds_july30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>473747.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>196</td>\n",
       "      <td>Soda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>473747.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12427</td>\n",
       "      <td>Original Beef Jerky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>473747.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10258</td>\n",
       "      <td>Pistachios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>473747.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25133</td>\n",
       "      <td>Organic String Cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>473747.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30450</td>\n",
       "      <td>Creamy Almond Butter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  order_id  order_number  add_to_cart_order  product_id  \\\n",
       "0      1.0  473747.0           3.0                1.0         196   \n",
       "1      1.0  473747.0           3.0                2.0       12427   \n",
       "2      1.0  473747.0           3.0                3.0       10258   \n",
       "3      1.0  473747.0           3.0                4.0       25133   \n",
       "4      1.0  473747.0           3.0                5.0       30450   \n",
       "\n",
       "            product_name  \n",
       "0                   Soda  \n",
       "1    Original Beef Jerky  \n",
       "2             Pistachios  \n",
       "3  Organic String Cheese  \n",
       "4   Creamy Almond Butter  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_products = df_product_embeddings.product_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49688\n"
     ]
    }
   ],
   "source": [
    "#number of unique products in dataset\n",
    "print(len(set(list_of_products)))\n",
    "n_products = len(set(list_of_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build skip-gram dataset\n",
    "def embeddings_pre_processing(categories, n_categories):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    \n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(categories).most_common(n_categories - 1))\n",
    "    most_common_category_dict = dict()\n",
    "    for category, _ in count:\n",
    "        most_common_category_dict[category] = len(most_common_category_dict)\n",
    "    \n",
    "    idx_categories = list()\n",
    "    unk_count = 0\n",
    "    for category in categories:\n",
    "        if category in most_common_category_dict:\n",
    "            index = most_common_category_dict[category]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        idx_categories.append(index)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reversed_most_common_category_dict = dict(zip(most_common_category_dict.values(),\n",
    "                                                   most_common_category_dict.keys()))\n",
    "    \n",
    "    return idx_categories, count, most_common_category_dict, reversed_most_common_category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_products, count_products, most_common_products_dict, reversed_most_common_products_dict = embeddings_pre_processing(list_of_products, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    \n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "embedding_size = 300  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a context.\n",
    "products_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x110c43b70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "num_sampled = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "        \n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    embeddings = tf.get_variable('embeddings_graph', [products_size, embedding_size])\n",
    "    embed_product_ids = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.get_variable('nce_weights', [products_size, embedding_size])\n",
    "    nce_biases = tf.get_variable('biases', [products_size], initializer=tf.zeros_initializer)\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed_product_ids,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=products_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create embeddings variable and lookup tensor\n",
    "# embeddings = tf.get_variable('embeddings_y', [20000, embedding_size])\n",
    "# embed_product_ids = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# #create weights and biases\n",
    "# weights = tf.get_variable('weights_embeds', [20000, embedding_size])\n",
    "# biases = tf.get_variable('biases', [20000], initializer=tf.zeros_initializer)\n",
    "# hidden_out = tf.matmul(embed_product_ids, tf.transpose(weights)) + biases\n",
    "\n",
    "# # convert train_context to a one-hot format\n",
    "# train_one_hot = tf.one_hot(train_labels, 20000)\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hidden_out, labels=train_one_hot))\n",
    "# # Construct the SGD optimizer using a learning rate of 0.1.\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cross_entropy)\n",
    "\n",
    "# norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "# normalized_embeddings = embeddings / norm\n",
    "# valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "# similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(graph, num_steps):\n",
    "    with tf.Session(graph = graph) as session:\n",
    "    \n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        #tf.global_variables_initializer().run()\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(idx_products, batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                  # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  13865.2353515625\n"
     ]
    }
   ],
   "source": [
    "final_embeds_similarity_test = get_embeddings(graph, num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
