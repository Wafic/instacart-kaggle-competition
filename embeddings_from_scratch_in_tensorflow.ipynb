{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following is heavily influenced by this [blogpost](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#data types\n",
    "import collections\n",
    "\n",
    "#import general libraries\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "#deep learning \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_products = pd.read_csv('./data/subset_products_list_july30.csv')\n",
    "df_product_embeddings = pd.read_csv('./data/subset_data_for_embeds_july30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_product_user = df_product_embeddings.groupby(['user_id','order_id']).agg({'product_id':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.085320529540459\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(count_product_user.product_id.mean())\n",
    "print(count_product_user.product_id.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    185274.000000\n",
       "mean         31.871094\n",
       "std          30.529451\n",
       "min           1.000000\n",
       "25%          11.000000\n",
       "50%          22.000000\n",
       "75%          43.000000\n",
       "max         418.000000\n",
       "Name: product_id, dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_product_user_unique = df_product_embeddings.groupby('user_id').agg({'product_id':'nunique'})\n",
    "count_product_user_unique.product_id.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_prior_df = pd.read_csv('./data/order_products__prior.csv', engine='c',\n",
    "                                          dtype={'order_id': np.int32, 'product_id': np.int32,\n",
    "                                                 'add_to_cart_order': np.int16, 'reordered': np.int8})\n",
    "\n",
    "order_products_train_df = pd.read_csv('./data/order_products__train.csv', engine='c',\n",
    "                                      dtype={'order_id': np.int32, 'product_id': np.int32,\n",
    "                                             'add_to_cart_order': np.int16, 'reordered': np.int8})\n",
    "\n",
    "orders_df = pd.read_csv('./data/orders.csv', engine='c',\n",
    "                        dtype={'order_id': np.int32, 'user_id': np.int32, 'order_number': np.int32,\n",
    "                               'order_dow': np.int8, 'order_hour_of_day': np.int8,\n",
    "                               'days_since_prior_order': np.float16})\n",
    "\n",
    "products_df = pd.read_csv(\"./data/products.csv\", engine='c')\n",
    "\n",
    "df_train = orders_df.merge(order_products_train_df, how='inner', on='order_id')\n",
    "df_train = df_train.merge(products_df, how='inner', on='product_id')\n",
    "df_train.sort_values(['user_id', 'order_number'], axis=0, inplace=True)\n",
    "\n",
    "df_prior = orders_df.merge(order_products_prior_df, how='inner', on='order_id')\n",
    "df_prior = df_prior.merge(products_df, how='inner', on='product_id')\n",
    "df_prior.sort_values(['user_id', 'order_number'], axis=0, inplace=True)\n",
    "\n",
    "df_products_orders_all = pd.concat([df_prior, df_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_orders_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_products = df_products_orders_all.product_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique products in dataset\n",
    "print(len(set(list_of_products)))\n",
    "n_products = len(set(list_of_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build skip-gram dataset\n",
    "def embeddings_pre_processing(categories, n_categories):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    \n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(categories).most_common(n_categories - 1))\n",
    "    most_common_category_dict = dict()\n",
    "    for category, _ in count:\n",
    "        most_common_category_dict[category] = len(most_common_category_dict)\n",
    "    \n",
    "    idx_categories = list()\n",
    "    unk_count = 0\n",
    "    for category in categories:\n",
    "        if category in most_common_category_dict:\n",
    "            index = most_common_category_dict[category]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        idx_categories.append(index)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reversed_most_common_category_dict = dict(zip(most_common_category_dict.values(),\n",
    "                                                   most_common_category_dict.keys()))\n",
    "    \n",
    "    return idx_categories, count, most_common_category_dict, reversed_most_common_category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_products, count_products, most_common_products_dict, reversed_most_common_products_dict = embeddings_pre_processing(list_of_products, n_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    \n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    \n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "embedding_size = 300  # Dimension of the embedding vector.\n",
    "skip_window = 3       # How many words to consider left and right.\n",
    "num_skips = 1         # How many times to reuse an input to generate a context.\n",
    "products_size = n_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 5     # Random set of words to evaluate similarity on.\n",
    "valid_window = 50  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "num_sampled = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "        \n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    embeddings = tf.get_variable('embeddings_graph', [products_size, embedding_size])\n",
    "    embed_product_ids = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.get_variable('nce_weights', [products_size, embedding_size])\n",
    "    nce_biases = tf.get_variable('biases', [products_size], initializer=tf.zeros_initializer)\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed_product_ids,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=products_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create embeddings variable and lookup tensor\n",
    "# embeddings = tf.get_variable('embeddings_y', [20000, embedding_size])\n",
    "# embed_product_ids = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# #create weights and biases\n",
    "# weights = tf.get_variable('weights_embeds', [20000, embedding_size])\n",
    "# biases = tf.get_variable('biases', [20000], initializer=tf.zeros_initializer)\n",
    "# hidden_out = tf.matmul(embed_product_ids, tf.transpose(weights)) + biases\n",
    "\n",
    "# # convert train_context to a one-hot format\n",
    "# train_one_hot = tf.one_hot(train_labels, 20000)\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hidden_out, labels=train_one_hot))\n",
    "# # Construct the SGD optimizer using a learning rate of 0.1.\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cross_entropy)\n",
    "\n",
    "# norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "# normalized_embeddings = embeddings / norm\n",
    "# valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "# similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(graph, num_steps):\n",
    "    with tf.Session(graph = graph) as session:\n",
    "    \n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        #tf.global_variables_initializer().run()\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(idx_products, batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                  # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(' ')\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "            if step % 4000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = reversed_most_common_products_dict[valid_examples[i]]\n",
    "                    top_k = 5  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = reversed_most_common_products_dict[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "                    print(' ')\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "product_embeddings_results = get_embeddings(graph, num_steps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df = pd.DataFrame.from_dict(reversed_most_common_products_dict,orient='index')\n",
    "index_df.columns = ['product_name']\n",
    "\n",
    "embeddings_w_product_name = pd.concat([index_df, pd.DataFrame(product_embeddings_results)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_w_product_name.to_csv('./data/sample__product_embeddings_results_long_trial.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
